{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9470756,"sourceType":"datasetVersion","datasetId":5759381},{"sourceId":10408208,"sourceType":"datasetVersion","datasetId":6449864},{"sourceId":10449374,"sourceType":"datasetVersion","datasetId":6468026},{"sourceId":10449644,"sourceType":"datasetVersion","datasetId":6468238},{"sourceId":10450932,"sourceType":"datasetVersion","datasetId":6469166},{"sourceId":10477433,"sourceType":"datasetVersion","datasetId":6487622}],"dockerImageVersionId":30823,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:18:22.727731Z","iopub.execute_input":"2025-01-15T14:18:22.728008Z","iopub.status.idle":"2025-01-15T14:18:23.105799Z","shell.execute_reply.started":"2025-01-15T14:18:22.727980Z","shell.execute_reply":"2025-01-15T14:18:23.105023Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/onlyragtruth/my_model_final-llama-onllyragtruth/adapter_model.safetensors\n/kaggle/input/onlyragtruth/my_model_final-llama-onllyragtruth/training_args.bin\n/kaggle/input/onlyragtruth/my_model_final-llama-onllyragtruth/adapter_config.json\n/kaggle/input/onlyragtruth/my_model_final-llama-onllyragtruth/README.md\n/kaggle/input/last-corpus/mushroom.zh-tst.v1.jsonl\n/kaggle/input/last-corpus/zh-last-query.json\n/kaggle/input/last-corpus/zh-now-query.json\n/kaggle/input/last-corpus/mushroom.en-tst.v1.jsonl\n/kaggle/input/last-corpus/en-now-query.json\n/kaggle/input/last-corpus/mushroom.zh-val.v2.jsonl\n/kaggle/input/last-corpus/en-last-query.json\n/kaggle/input/last-corpus/mushroom.en-val.v2.jsonl\n/kaggle/input/text-en/mushroom.en-val.v1.jsonl\n/kaggle/input/review2-0/en-now-query.json\n/kaggle/input/modellora/my_model_halu-rag--llama 3b/adapter_model.safetensors\n/kaggle/input/modellora/my_model_halu-rag--llama 3b/training_args.bin\n/kaggle/input/modellora/my_model_halu-rag--llama 3b/adapter_config.json\n/kaggle/input/modellora/my_model_halu-rag--llama 3b/README.md\n/kaggle/input/modellora/my_model_halueval-llama 3b/adapter_model.safetensors\n/kaggle/input/modellora/my_model_halueval-llama 3b/training_args.bin\n/kaggle/input/modellora/my_model_halueval-llama 3b/adapter_config.json\n/kaggle/input/modellora/my_model_halueval-llama 3b/README.md\n/kaggle/input/modellora/my_model_-rag-halueval-llama 3b/adapter_model.safetensors\n/kaggle/input/modellora/my_model_-rag-halueval-llama 3b/training_args.bin\n/kaggle/input/modellora/my_model_-rag-halueval-llama 3b/adapter_config.json\n/kaggle/input/modellora/my_model_-rag-halueval-llama 3b/README.md\n/kaggle/input/modellora/my_model-ragl-llama 3b/adapter_model.safetensors\n/kaggle/input/modellora/my_model-ragl-llama 3b/training_args.bin\n/kaggle/input/modellora/my_model-ragl-llama 3b/adapter_config.json\n/kaggle/input/modellora/my_model-ragl-llama 3b/README.md\n/kaggle/input/review/en-last-query.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!pip install peft\n!pip install accelerate\n!pip install -U bitsandbytes","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\nimport torch\nfrom datasets import load_dataset\nfrom transformers import AutoTokenizer, XLNetTokenizerFast,LlamaModel,LlamaTokenizerFast,PreTrainedTokenizerFast,BitsAndBytesConfig\nimport huggingface_hub \nfrom transformers import LlamaForCausalLM\nfrom transformers import Trainer, TrainingArguments\n#from sklearn.model_selection import train_test_split\nimport json\nfrom huggingface_hub import login\nimport torch\nfrom peft import LoraConfig,get_peft_model\nimport wandb\n\ncompute_dtype = getattr(torch, \"float16\")\nbnb_config = BitsAndBytesConfig(\n        load_in_4bit=True,\n        bnb_4bit_quant_type=\"nf4\",\n        bnb_4bit_compute_dtype=compute_dtype,\n        bnb_4bit_use_double_quant=True,\n)\nlora_config = LoraConfig(\n    r = 16,\n    lora_alpha=32,\n    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"TOKEN_CLS\"     \n)\n\n\n\n\ntokenizer = LlamaTokenizerFast.from_pretrained('meta-llama/Llama-3.2-3B')\ntokenizer.pad_token = tokenizer.decode(128004)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\nimport os\n\n\nimport joblib\nimport numpy as np\nimport torch\nfrom datasets import Dataset\nfrom transformers import AutoTokenizer, AutoModel\n\n\n\nwith open('/kaggle/input/last-corpus/mushroom.zh-val.v2.jsonl', 'r', encoding='utf-8') as file:\n    data = []\n    for line in file:\n        data.append(json.loads(line.strip()))\nwith open('/kaggle/input/last-corpus/zh-last-query.json', \"r\", encoding=\"utf-8\") as json_file:\n    queries = json.load(json_file)\nfor i in range(len(data)):\n    data[i]['context'] = queries.get(data[i]['model_input'])\ntrain = Dataset.from_list(data)\ntrain =train.remove_columns(['lang','model_id','model_output_logits','model_output_tokens'])\n\n\nend_token = tokenizer.decode(128001)\nstart_token = tokenizer.decode(128000)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:18:59.206532Z","iopub.execute_input":"2025-01-15T14:18:59.207056Z","iopub.status.idle":"2025-01-15T14:18:59.275131Z","shell.execute_reply.started":"2025-01-15T14:18:59.207034Z","shell.execute_reply":"2025-01-15T14:18:59.274550Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"dataset = train.map(preprocess,batched=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:18:59.275921Z","iopub.execute_input":"2025-01-15T14:18:59.276193Z","iopub.status.idle":"2025-01-15T14:18:59.605140Z","shell.execute_reply.started":"2025-01-15T14:18:59.276172Z","shell.execute_reply":"2025-01-15T14:18:59.604239Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/50 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e103aa4256f43b4887ed6cd830a54fa"}},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"id2label = {\n    0: 'B-HAL',  \n    1: 'I-HAL',  \n    2: 'E-HAL',  \n    3: 'O'       \n}\nlabel2id = {\n    'B-HAL': 0,\n    'I-HAL': 1,\n    'E-HAL': 2,\n    'O': 3\n}\nfrom transformers import LlamaForTokenClassification,AutoModelForTokenClassification\nfrom peft import PeftModel\n\nmodel = LlamaForTokenClassification.from_pretrained('meta-llama/Llama-3.2-3B', num_labels=4, id2label=id2label, label2id=label2id,output_hidden_states=True)\n\nmodel = PeftModel.from_pretrained(model,\"/kaggle/input/modellora/my_model-ragl-llama 3b\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:18:59.607023Z","iopub.execute_input":"2025-01-15T14:18:59.607271Z","iopub.status.idle":"2025-01-15T14:21:40.817299Z","shell.execute_reply.started":"2025-01-15T14:18:59.607241Z","shell.execute_reply":"2025-01-15T14:21:40.816342Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/844 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"88c15238e71344adaaf6a0e9a560bb34"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/20.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec187dfa714b4df583e0971158341412"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f5b61d644bb43abbbe77f2a11228570"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"585804a912d84bae96c04db0d2cb1c1f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.46G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"44c374da9c5e4094994e9bbcf3766778"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"779b0afa80a3494bb115e1074ba264f1"}},"metadata":{}},{"name":"stderr","text":"Some weights of LlamaForTokenClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B and are newly initialized: ['score.bias', 'score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"def preprocess(tmp):\n\n\n    input_text = f\"{tmp.get('model_input')}{end_token}{start_token}{tmp.get('context')}{end_token}\"\n    text_pair = f\"{tmp.get('model_output_text')}\"\n    inputs = tokenizer(text=input_text, text_pair=text_pair, return_tensors='pt',\n                       return_offsets_mapping=True).to('cuda')\n\n    inputs['input_ids']=inputs['input_ids']\n    inputs['attention_mask']=inputs['attention_mask']\n    tmp = inputs.encodings[0].offsets\n    return tmp,inputs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:21:46.806409Z","iopub.execute_input":"2025-01-15T14:21:46.806718Z","iopub.status.idle":"2025-01-15T14:21:46.811222Z","shell.execute_reply.started":"2025-01-15T14:21:46.806690Z","shell.execute_reply":"2025-01-15T14:21:46.810415Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"with open('/kaggle/input/last-corpus/mushroom.en-tst.v1.jsonl', 'r', encoding='utf-8') as file:\n    data = []\n    for line in file:\n        data.append(json.loads(line.strip()))\nwith open('/kaggle/input/review2-0/en-now-query.json', \"r\", encoding=\"utf-8\") as json_file:\n    queries = json.load(json_file)\nfor i in range(len(data)):\n    data[i]['context'] = queries.get(data[i]['model_input'])\ntest = Dataset.from_list(data)\ntest =test.remove_columns(['id','lang','model_id','model_output_logits','model_output_tokens'])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:21:48.860115Z","iopub.execute_input":"2025-01-15T14:21:48.860480Z","iopub.status.idle":"2025-01-15T14:21:48.886245Z","shell.execute_reply.started":"2025-01-15T14:21:48.860451Z","shell.execute_reply":"2025-01-15T14:21:48.885399Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import torch.nn.functional as F\nmodel.to('cuda')\ndef softmax(logits):\n    \n    probabilities = F.softmax(logits,dim=0)  \n    return probabilities\nwait = [] \nwith torch.no_grad():\n    for i in range(len(test)):\n        mapp,ret = preprocess(test[i])\n        ret.pop('offset_mapping')\n        mapp = mapp\n        \n        ret= model(**ret).logits\n        predicted_token_class_ids = ret.argmax(-1)\n        prob= ret[0]\n        j =len(ret)-1\n        \n        predicted_tokens_classes = [id2label[t.item()] for t in predicted_token_class_ids[0]]\n        labels = predicted_token_class_ids[0].tolist()\n        i = len(mapp)-1\n        ret = []\n        tmp =[]\n        \n        while i>0:\n            if mapp[i][0]==0 and mapp[i][1]==0:\n                break\n            if labels[i]!=3:\n                if len(tmp)==0:\n                    tmp.append(mapp[i][0])\n                    tmp.append(mapp[i][1])\n                    \n                else:\n                    tmp[0] = mapp[i][0]\n                    \n                    if labels[i]==0:\n                        \n                        probabilities = softmax(prob[j])\n                        tmp.append(probabilities[probabilities.argmax(-1)])\n                        ret.append(tmp)\n                        tmp = []\n            else:\n                if len(tmp)>0:\n                    \n                    probabilities = softmax(prob[j-1])\n                    tmp.append(probabilities[probabilities.argmax(-1)])\n                    ret.append(tmp)\n                    tmp = []\n            i-=1\n            j-=1\n        wait.append(ret)    \n        \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:22:57.166631Z","iopub.execute_input":"2025-01-15T14:22:57.167207Z","iopub.status.idle":"2025-01-15T14:23:42.059089Z","shell.execute_reply.started":"2025-01-15T14:22:57.167138Z","shell.execute_reply":"2025-01-15T14:23:42.058404Z"}},"outputs":[{"name":"stderr","text":"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"with open('/kaggle/input/last-corpus/mushroom.en-tst.v1.jsonl', 'r', encoding='utf-8') as file:\n    data = []\n    for line in file:\n        data.append(json.loads(line.strip()))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-15T14:23:55.251433Z","iopub.execute_input":"2025-01-15T14:23:55.251736Z","iopub.status.idle":"2025-01-15T14:23:55.268966Z","shell.execute_reply.started":"2025-01-15T14:23:55.251712Z","shell.execute_reply":"2025-01-15T14:23:55.268331Z"}},"outputs":[],"execution_count":16}]}